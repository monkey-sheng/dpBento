{
    "dpbento_root": "/optional/path/to/your/DPU-bench",
    "output_dir": "/optional/path/to/your/DPU-bench/experiments/storage_test/Output",
    "user_benchmarks_dir": "/optional/path/to/user_benchmarks",
    
    "benchmarks":
    [
        {
            // we should allow the user to specify only the benchmark class, and default to all items in that class
            "benchmark_class": "storage",

            // the names of the benchmarks go in here, if it's missing let's assume we include all benchmarks
            "benchmark_items": ["fio", "something_else"],
            "parameters": {
                // all these parameters are shared by all the benchmark items in the same benchmark class
                "numProc": [1,2,4,6,8],
                "block_sizes": ["1m"],
                "size": ["1G"],
                "runtime": ["35s"],
                "direct": [1],
                "iodepth": [2,4,8,16,32],
                "io_engine": ["io_uring"],
                "test_lst": ["randread"]
            },
            "metrics": ["latency", "bandwidth", "IOPS"],
            // not sure how we gonna use it, but it should be benchmark name as keys and list of hints as values
            "report_hints": {
                "plot": "bar_plot",
                "x": "block_sizes",
                "y": ["latency", "IOPS"],
                // group the bars, can be a list or a dict where values are specific values of the parameter
                // this example is a list: for each y attribute to plot (i.e. latency and IOPS),
                // group by numProc and iodepth, then apply max() on the values
                "group_by": {
                    "attributes": ["numProc", "iodepth"],
                    "aggregate": "max"  // aggregation applied on the y value(s) of different runs
                }
            }
        },
        {
            "benchmark_class": "communication",
            "benchmark_items": ["DMA", "RDMA", "TCP"],
            "parameters": {
                "data_size": [8, 16, 64, 128, 512, 1024, 2048, 4096, 8192],
                "queue_depth": [1, 2, 4, 8, 16],  // the code currently allows scalar values, or lists
                "number_of_threads": [1, 2, 4, 10, 20],
                // IP addr doesn't apply to DMA, the DMA benchmark item script will still receive these,
                // but during argparsing it should ignore them, i.e. `parse_known_args()` should be used
                "test_rounds":[100, 1000, 10000],
                "host_pci": "e1:00.1",
                "host_ip": "10.10.1.2",
                "dpu_ip": "10.10.1.42"
            },
            "metrics": ["latency", "throughput"],

            // hints is needed to produce the plot for each benchmark item
            "report_hints": {
                "x": "data_size",  // the x-axis of the plot, can be overridden, see below

                // this will produce two separate plots, one for latency and one for throughput
                "y": ["latency", "throughput"],  // the y-axis of the plot, can be overridden, see below

                // the type of plot (should come from a fixed list probably), can be overridden, see below
                "plot":"line_plot",

                // specific hints for a benchmark item
                // for example, we want to compare DMA latency with RDMA,
                // plotting two bars in a single plot
                "DMA": {
                    "plot": "bar_plot",  // override the plot type
                    "x": "data_size",  // this needs to match the parameter name
                    "y": ["latency"],  // this comes from `metrics`

                    // reference another benchmark item(s), the same plot type, x, y, etc. will be used
                    "compare": ["RDMA"],  // the other benchmark item(s) to compare with (i.e. seaborn param of hue)

                    "group_by": {
                        "attributes": ["numProc", "iodepth"],
                        "aggregate": "avg",  // aggregation applied on the y value(s) of different runs
                        
                        // optionally, select a single (aggregated, again) value from the aggregated values
                        // without select, all groups will be plotted, as n bars for each x value
                        // after select we use only 1 single value, i.e. a single bar,
                        // this is probably required for comparison
                        "select": "max"  // use only the `select`ed (i.e. max) y value after grouping
                    }

                },
                "TCP": {
                    "plot": "bar_plot",
                    "x": "data_size",
                    "y": "latency"  // allow non-list value

                }
            }
        }
    ]
}